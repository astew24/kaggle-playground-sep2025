{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KAGGLE PLAYGROUND SERIES S3E9 - CONCRETE STRENGTH PREDICTION\n",
    "# =============================================================================\n",
    "# This notebook implements a machine learning pipeline to predict concrete \n",
    "# compressive strength based on various material properties and mix ratios.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train = pd.read_csv(\"data/train.csv\")  # Changed to local path\n",
    "test = pd.read_csv(\"data/test.csv\")    # Changed to local path\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "print(f\"Training set shape: {train.shape}\")\n",
    "print(f\"Test set shape: {test.shape}\")\n",
    "print(f\"Submission template shape: {submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"\\n1. TRAINING DATA INFO:\")\n",
    "print(f\"Shape: {train.shape}\")\n",
    "print(f\"Columns: {list(train.columns)}\")\n",
    "\n",
    "print(\"\\n2. FIRST FEW ROWS:\")\n",
    "display(train.head())\n",
    "\n",
    "print(\"\\n3. DATA TYPES:\")\n",
    "print(train.dtypes)\n",
    "\n",
    "print(\"\\n4. STATISTICAL SUMMARY:\")\n",
    "display(train.describe())\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"\\n5. MISSING VALUES ANALYSIS:\")\n",
    "missing = train.isnull().sum()\n",
    "missing_pct = (missing / len(train)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing Percentage': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Target variable analysis\n",
    "print(\"\\n6. TARGET VARIABLE ANALYSIS:\")\n",
    "print(f\"Target column: 'target'\")\n",
    "print(f\"Target range: {train['target'].min():.2f} - {train['target'].max():.2f}\")\n",
    "print(f\"Target mean: {train['target'].mean():.2f}\")\n",
    "print(f\"Target std: {train['target'].std():.2f}\")\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train['target'], kde=True, bins=50)\n",
    "plt.title(\"Target Distribution\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Target Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=train['target'])\n",
    "plt.title(\"Target Box Plot\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"Target Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BASELINE MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"BASELINE MODEL TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare features and target\n",
    "print(\"1. PREPARING DATA...\")\n",
    "X = train.drop(columns=[\"target\", \"id\"])  # Remove target and ID columns\n",
    "y = train[\"target\"]\n",
    "\n",
    "print(f\"Feature columns: {list(X.columns)}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "\n",
    "# Select only numeric features for baseline\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "print(f\"Numeric features selected: {X_numeric.shape[1]}\")\n",
    "\n",
    "# Train-validation split\n",
    "print(\"\\n2. SPLITTING DATA...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_numeric, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=None  # No stratification for regression\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "\n",
    "# Initialize and train baseline model\n",
    "print(\"\\n3. TRAINING BASELINE MODEL...\")\n",
    "print(\"Model: Random Forest Regressor\")\n",
    "print(\"Parameters: n_estimators=100, random_state=42\")\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n4. MAKING PREDICTIONS...\")\n",
    "train_preds = model.predict(X_train)\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\n5. EVALUATION METRICS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Training metrics\n",
    "train_rmse = mean_squared_error(y_train, train_preds, squared=False)\n",
    "train_mae = mean_absolute_error(y_train, train_preds)\n",
    "train_r2 = r2_score(y_train, train_preds)\n",
    "\n",
    "print(f\"TRAINING SET:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  MAE:  {train_mae:.4f}\")\n",
    "print(f\"  R²:   {train_r2:.4f}\")\n",
    "\n",
    "# Validation metrics\n",
    "val_rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "val_mae = mean_absolute_error(y_val, val_preds)\n",
    "val_r2 = r2_score(y_val, val_preds)\n",
    "\n",
    "print(f\"\\nVALIDATION SET:\")\n",
    "print(f\"  RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  MAE:  {val_mae:.4f}\")\n",
    "print(f\"  R²:   {val_r2:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "overfitting = train_rmse - val_rmse\n",
    "print(f\"\\nOverfitting check (Train RMSE - Val RMSE): {overfitting:.4f}\")\n",
    "if overfitting > 0.1:\n",
    "    print(\"⚠️  Warning: Potential overfitting detected!\")\n",
    "else:\n",
    "    print(\"✅ Model appears to generalize well\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\n6. FEATURE IMPORTANCE (Top 10):\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_numeric.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEXT STEPS FOR IMPROVEMENT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"POTENTIAL IMPROVEMENTS TO TRY:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. Feature Engineering:\")\n",
    "print(\"   - Create interaction features\")\n",
    "print(\"   - Polynomial features\")\n",
    "print(\"   - Domain-specific ratios\")\n",
    "print()\n",
    "print(\"2. Model Improvements:\")\n",
    "print(\"   - Try XGBoost, LightGBM, or CattBoost\")\n",
    "print(\"   - Hyperparameter tuning\")\n",
    "print(\"   - Ensemble methods\")\n",
    "print()\n",
    "print(\"3. Advanced Techniques:\")\n",
    "print(\"   - Cross-validation\")\n",
    "print(\"   - Feature selection\")\n",
    "print(\"   - Outlier detection and handling\")\n",
    "print()\n",
    "print(\"4. Data Analysis:\")\n",
    "print(\"   - Correlation analysis\")\n",
    "print(\"   - Feature distributions\")\n",
    "print(\"   - Target vs feature relationships\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
